{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BetterNN(nn.Module):\n",
    "    \"\"\"\n",
    "    This is a better neural network model ready to use feature vector\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size=38, output_size=18):\n",
    "        super(BetterNN, self).__init__()\n",
    "        \n",
    "        # use relu as activation function, cause any other is not suitable for this task\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "        # Conv1d layer\n",
    "        self.conv1d = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=3)\n",
    "        \n",
    "        # Calculate the size after the Conv1d layer\n",
    "        # Assuming input_size is the length of the sequence and the input is of shape (batch_size, 1, input_size)\n",
    "        conv_output_size = (input_size - 3 + 1)  # since kernel_size=3 and stride=1, padding=0\n",
    "        \n",
    "        # First linear layer should take conv_output_size as input\n",
    "        self.fc1 = nn.Linear(conv_output_size, 200)\n",
    "        \n",
    "        # the hidden part of linear layers\n",
    "        self.fc2 = nn.Linear(200, 500)  # Change input size according to fc1 output\n",
    "        self.fc3 = nn.Linear(500, 500)\n",
    "        self.fc4 = nn.Linear(500, 300)\n",
    "        self.fc5 = nn.Linear(300, 100)\n",
    "        \n",
    "        # The output layer for classification\n",
    "        self.fc6 = nn.Linear(100, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Reshape x to fit Conv1d input requirements\n",
    "        x = x.unsqueeze(0)  # Add a channel dimension: (batch_size, 1, input_size)\n",
    "        x = self.conv1d(x)  # Apply Conv1d\n",
    "        x = x.squeeze(1)     # Remove the channel dimension after conv: (batch_size, conv_output_size)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc5(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc6(x)\n",
    "        return x\n",
    "    \n",
    "model = BetterNN().to(device)\n",
    "test_input = torch.randn(38).to(device)\n",
    "# print(model(test_input.unsqueeze(0)))\n",
    "# print(model(test_input.unsqueeze(0)))\n",
    "# print(test_input)\n",
    "res = model(test_input)\n",
    "# print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model(test_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ATCDataset_v2(Dataset):\n",
    "    \n",
    "    def __init__(self, in_data_path, out_data_path):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_path (str): path to the data in csv format\n",
    "        \"\"\"\n",
    "        self.df = pd.read_csv(\n",
    "            in_data_path,\n",
    "            delimiter=\",\",\n",
    "            header=0\n",
    "            )\n",
    "        self.out_df = pd.read_csv(\n",
    "            out_data_path,\n",
    "            delimiter=\",\",\n",
    "            header=0\n",
    "            )\n",
    "        \n",
    "        # build OUTPUT DATA for the loss function\n",
    "        # ========================================\n",
    "        # labels for vector of the ouput boxes \n",
    "        labels = [\n",
    "            \"OM0000\", \"OM0001\", \"OM0002\", \"OM0003\", \"OM0004\", \"OM0005\", \"OM0006\", \"OM0007\", \n",
    "            \"OM0008\", \"OM0009\", \"OM0010\", \"PAL01\", \"PAL02\", \"PAL03\", \"PAL04\", \"PAL05\", \"PAL06\", \n",
    "            \"PAL07\"\n",
    "        ]\n",
    "\n",
    "        # result vector contains number of occurences of each box for each group of items\n",
    "        result_vector = {}\n",
    "        for group in self.out_df['GroupDelivery'].unique():\n",
    "            result_vector[group] = np.zeros((labels.__len__()))\n",
    "            \n",
    "        for _, row in self.out_df.iterrows():\n",
    "            cartonName = row['UsedCarton'].strip().upper()\n",
    "            result_vector[row['GroupDelivery']][labels.index(cartonName)] += 1\n",
    "        self.result_vector = result_vector\n",
    "        \n",
    "        # build INPUT DATA \n",
    "        # ========================================\n",
    "        \n",
    "        # Apply the function to each GroupDelivery\n",
    "        def vectorize(group):\n",
    "            return group[['X', 'Y', 'Z', 'Weight', 'Qty']].values.flatten().tolist()\n",
    "        \n",
    "        grouped = self.df.groupby('GroupDelivery').apply(vectorize, include_groups=False).reset_index()\n",
    "\n",
    "        \n",
    "        # Rename columns\n",
    "        grouped.columns = ['GroupDelivery', 'Vector']\n",
    "        \n",
    "        # store vectors and group delivery\n",
    "        self.vectors = grouped.Vector\n",
    "        self.groupDelivery = grouped.GroupDelivery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_IN=\"data/test_in.csv\"\n",
    "TEST_OUT=\"data/test_out.csv\"\n",
    "\n",
    "data_test = ATCDataset_v2(TEST_IN, TEST_OUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is create feature vector function\n",
    "# it creates from the group of items a single vector, that describes most the group\n",
    "def create_feature_vector(group):\n",
    "    gr = group[['X','Y','Z','Weight']].copy()\n",
    "    gr_all = group[['X','Y','Z','Weight','Qty']].copy()\n",
    "    \n",
    "    # get the sum of the values\n",
    "    x_sum,y_sum,z_sum,w_sum = gr.sum(numeric_only=True, axis=0)\n",
    "    # get the mean of the values\n",
    "    x_mean, y_mean, z_mean, w_mean = gr.mean(numeric_only=True, axis=0)\n",
    "    # get the standard deviation of the values\n",
    "    x_std, y_std, z_std, w_std = gr.std(numeric_only=True, axis=0)\n",
    "    # get the median of the values\n",
    "    x_median, y_median, z_median, w_median = gr.median(numeric_only=True, axis=0)\n",
    "    # get qty of items to boxes according to the weight\n",
    "    bins_weight = [0, 1, 3, 6, 9, 13, 20, 25, 30, 100]  # Example weight ranges for bins\n",
    "    labels_weight = ['0-1kg', '1-3kg', '3-6kg', '6-9kg','9-13kg','13-20kg','20-25kg','25-30kg','30-nkg']  # Labels for bins\n",
    "    gr_all.loc[:,'Weight_bin'] = pd.cut(gr['Weight'], bins=bins_weight, labels=labels_weight, right=False)\n",
    "    # get qty of items to boxes according to the volume\n",
    "    gr.loc[:,'Volume'] = gr['X']*gr['Y']*gr['Z']\n",
    "    \n",
    "    bins_volume = [0,1000,2000,5000,10000,30000,50000,80000,100000,150000,250000,400000,600000,800000,1000000,10000000] # cm3\n",
    "    labels_volume = [ \n",
    "        '0-1dm3','1-2dm3','2-5dm3','5-10dm3', '10-30dm3', '30-50dm3', '50-80dm3', '80-100dm3', \n",
    "        '100-150dm3', '150-250dm3', '250-400dm3', '400-600dm3', \n",
    "        '600-800dm3', '800-1000dm3', '1000-10000dm3'\n",
    "    ]  # Labels for bins (written in dm3 for better readability)\n",
    "    gr_all.loc[:,'Volume_bin'] = pd.cut(gr['Volume'], bins=bins_volume, labels=labels_volume, right=False)\n",
    "\n",
    "    # return the values as a pandas series\n",
    "    feature_vector = pd.Series(\n",
    "        [\n",
    "            x_sum, y_sum, z_sum, w_sum,\n",
    "            x_mean, y_mean, z_mean, w_mean,\n",
    "            x_std, y_std, z_std, w_std,\n",
    "            x_median, y_median, z_median, w_median\n",
    "        ], \n",
    "        index=[\n",
    "            'X_sum', 'Y_sum', 'Z_sum', 'Weight_sum',\n",
    "            'X_mean', 'Y_mean', 'Z_mean', 'Weight_mean',\n",
    "            'X_std', 'Y_std', 'Z_std', 'Weight_std',\n",
    "            'X_median', 'Y_median', 'Z_median', 'Weight_median'\n",
    "        ])\n",
    "    \n",
    "    # Get the counts of each weight bin\n",
    "    weight_bin_qty_sum = gr_all.groupby('Weight_bin',observed=True)['Qty'].sum()\n",
    "    volume_bin_qty_sum = gr_all.groupby('Volume_bin',observed=True)['Qty'].sum()\n",
    "    # Append the weight bin counts to the feature vector\n",
    "    for bin_label in labels_weight:\n",
    "        feature_vector[f'bin_{bin_label}_count'] = weight_bin_qty_sum.get(bin_label, 0)\n",
    "    for bin_label in labels_volume:\n",
    "        feature_vector[f'bin_{bin_label}_count'] = volume_bin_qty_sum.get(bin_label, 0)\n",
    "    return feature_vector.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = data_test.df.groupby('GroupDelivery',observed=True).apply(func=create_feature_vector, include_groups=False).reset_index()\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one = data_test.df.groupby('GroupDelivery').get_group(12022211)\n",
    "res = create_feature_vector(one) \n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(start=0, stop=data_test.df.shape[0], step=1)\n",
    "y = np.array([0]*data_test.df.shape[0])\n",
    "for i,s in data_test.df.iterrows():\n",
    "    y[i] = s.X*s.Y*s.Z\n",
    "    if (y[i] > 3000000):\n",
    "        print(s)\n",
    "        print(i)\n",
    "        print(y[i])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "ax = plt.plot(x, y)\n",
    "plt.ylim(0, 1000000)\n",
    "plt.yticks(np.arange(0, 1000000, step=100000))\n",
    "bins_volume = [0,10000,30000,50000,80000,100000,150000,250000,400000,600000,800000,1000000,10000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Generate data\n",
    "x = np.linspace(0, 10, 100)\n",
    "y1 = np.sin(x)\n",
    "y2 = np.cos(x)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure()\n",
    "\n",
    "# Plot the first graph (sine)\n",
    "plt.plot(x, y1, label='Sine', color='b')\n",
    "\n",
    "# Plot the second graph (cosine)\n",
    "plt.plot(x, y2, label='Cosine', color='r')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('X-axis')\n",
    "plt.ylabel('Y-axis')\n",
    "plt.title('Sine and Cosine Waves')\n",
    "\n",
    "# Add a legend to differentiate the two graphs\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
