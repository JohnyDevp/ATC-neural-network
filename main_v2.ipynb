{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import atc_dataloader, atc_model\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from atc_loss import PredictionLoss_COS_MSE, PredictionLoss_BOX_Wise\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_vector_to_integers(pred):\n",
    "    # //////////////////////////////////////////////////////////\n",
    "    # CURRENTLY NOT USED\n",
    "    # //////////////////////////////////////////////////////////\n",
    "    # get the max value, that will serve as reference - where is the max, there is supposed to be 1 (or other integer, depends...)\n",
    "    # for now, everything until 1.3 will be treated as 1, until 2.3 as two etc.\n",
    "    \n",
    "    # lambda to round function from the specific treshold defined in range [.0;1.0)\n",
    "    my_treshold = 0.3\n",
    "    tr = np.min([my_treshold, pred.max()*0.9])\n",
    "\n",
    "    myround = np.vectorize(lambda x, treshold=tr: np.floor(x) if x < (np.floor(x) + treshold) else np.ceil(x))\n",
    "\n",
    "    result = np.apply_along_axis(func1d=myround, axis=0,arr=pred)\n",
    "    return result\n",
    "\n",
    "def evaluate(model, data_loader, device, criterion):\n",
    "    \"\"\"\n",
    "    function used to evaluate the model, return loss and accuracy\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    test_loss = []\n",
    "    with torch.no_grad():\n",
    "        for  _, batch_in_vectors, batch_out_vectors in data_loader:\n",
    "            pred = model(batch_in_vectors.to(device))\n",
    "            loss = criterion(pred, batch_out_vectors.to(device))\n",
    "            test_loss.append(loss.item())\n",
    "    \n",
    "    return np.mean(test_loss)\n",
    "\n",
    "# def evaluate_real_similarity(dl_test,model,device):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_vector_similarity(vec1, vec2, tolerance=0.1):\n",
    "    # Convert to numpy arrays for element-wise comparison\n",
    "    vec1 = np.array(vec1)\n",
    "    vec2 = np.array(vec2)\n",
    "    \n",
    "    # Compute the absolute difference between corresponding elements\n",
    "    difference = np.abs(vec1 - vec2)\n",
    "    print(difference)\n",
    "    # Check if all differences are within the given tolerance\n",
    "    return np.all(difference <= tolerance)\n",
    "\n",
    "real_vector_similarity([[1.0, 2.0, 3.0],[5.09, 6.0, 7.0]], [[1.0, 2.0, 3.0],[5.3, 6.0, 7.0]], tolerance=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, device, train_dl, test_dl, criterion, optimizer, epochs=10, save_model_last_epoch=True, model_path='.'):\n",
    "    \n",
    "    # loss that will be calculated after each epoch for both test and train set \n",
    "    train_loss_overall = []\n",
    "    test_loss_overall = [] \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = []\n",
    "        model.train()  # Set the model to training mode\n",
    "        \n",
    "        # Wrap dataloader with tqdm for progress visualization\n",
    "        for _, batch_in_vectors, batch_out_vectors in tqdm(train_dl, desc=f'Epoch {epoch + 1}/{epochs}', unit='batch'):\n",
    "            # Convert data to tensors and move to the correct device\n",
    "            batch_in_vectors = torch.tensor(batch_in_vectors, dtype=torch.float32, requires_grad=True).to(device)\n",
    "            batch_out_vectors = torch.tensor(batch_out_vectors, dtype=torch.float32).to(device)\n",
    "            \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(batch_in_vectors)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, batch_out_vectors)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate loss\n",
    "            epoch_loss.append(loss.item())\n",
    "\n",
    "        # start evaluation and append test loss\n",
    "        tmp_test_loss = evaluate(model, test_dl, device, criterion)\n",
    "        test_loss_overall.append(tmp_test_loss)\n",
    "        \n",
    "        # Calculate average train loss for the epoch\n",
    "        avg_train_loss = np.mean(epoch_loss)\n",
    "        train_loss_overall.append(avg_train_loss) # add it to the loss over all epochs\n",
    "        \n",
    "        # Print loss (both train and test) for the current epoch\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Train loss: {avg_train_loss:.4f}, Test loss: {tmp_test_loss:.4f}\")\n",
    "\n",
    "    # save the model after the last epoch if set\n",
    "    if (save_model_last_epoch):\n",
    "        torch.save(model.state_dict(), f'{model_path}/model_last_epoch_{time.time_ns()}.pth')\n",
    "        \n",
    "    # print overall loss\n",
    "    print('Overall train loss: ', train_loss_overall)\n",
    "    print('Overall test loss: ', test_loss_overall)\n",
    "    # plot graph\n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(0,epochs,1),train_loss_overall, label='Train loss') # train loss over all epochs\n",
    "    plt.plot(np.arange(0,epochs,1),test_loss_overall, label='Test loss') # test loss over all epochs\n",
    "    plt.title('Loss function')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "# TRAIN_IN=\"data/train_in.csv\"\n",
    "# TRAIN_OUT=\"data/train_out.csv\"\n",
    "TRAIN_IN='data/sample_in.csv'\n",
    "TRAIN_OUT='data/sample_out.csv'\n",
    "data_train = atc_dataloader.ATCDataset_v2(TRAIN_IN, TRAIN_OUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST_IN=\"data/test_in.csv\"\n",
    "# TEST_OUT=\"data/test_out.csv\"\n",
    "TEST_IN='data/sample_in.csv'\n",
    "TEST_OUT='data/sample_out.csv'\n",
    "data_test = atc_dataloader.ATCDataset_v2(TEST_IN, TEST_OUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = atc_model.BetterNN().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001, weight_decay=0.005)\n",
    "criterion = PredictionLoss_BOX_Wise().to(device)\n",
    "dl_train = DataLoader(data_train, batch_size=32, shuffle=True)  \n",
    "dl_test = DataLoader(data_test, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, device, dl_train, dl_test, criterion, optimizer, epochs=1)\n",
    "torch.save(model.state_dict(), f'models/model_basenn{time.time_ns()}.pth') # it will be saved, ...but just in case (rly doesnt make much sense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, dl_test, device, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, device, dl_train, criterion, optimizer, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with already trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the weights to the model\n",
    "model = atc_model.BaseNN().to(device)\n",
    "model.load_state_dict(torch.load('models/model_basenn.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 2\n",
    "target = data_train.__getitem__(idx)[2]\n",
    "pred = model(data_train.__getitem__(idx)[1]) \n",
    "print(target)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the prediction loss\n",
    "cos = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "mse = nn.MSELoss()\n",
    "    \n",
    "# Define weights for functions for Cos and MSE.\n",
    "w1 = 5\n",
    "w2 = 10\n",
    "cc = cos(torch.tensor([.1,3,4]), torch.tensor([.1,3,4]))\n",
    "# cc = cos(torch.cumsum(pred, dim=-1), torch.cumsum(target, dim=-1))\n",
    "print(cc)\n",
    "# Apply cumulative sum to both tensors and calculate loss.\n",
    "cos_sim = torch.abs(cos(torch.cumsum(pred, dim=-1), torch.cumsum(target, dim=-1))).mean()\n",
    "mse_loss = mse(torch.cumsum(pred, dim=-1), torch.cumsum(target, dim=-1))\n",
    "loss = (w1 * mse_loss) / (w2 * cos_sim)\n",
    "# penalty for the number of places, where prediction misses the zero in target\n",
    "# let say where the \n",
    "non_zero_indices_target = np.count_nonzero(target)\n",
    "max_value_pred = torch.max(pred)\n",
    "# now everywhere the value goes up the half of max_value_pred penalty is count  --- this is self defined rule\n",
    "\n",
    "print(non_zero_indices_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_ok = 0\n",
    "for d in data_train:\n",
    "    if np.argmax(d[2]) == torch.argmax(model(d[1])):\n",
    "        total_ok += 1\n",
    "print(total_ok, total_ok/len(data_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=np.sum([0.0807, 0.0806, 0.0767, 0.0896, 0.0916, 0.0810, 0.0730, 0.0753, 0.0657,\n",
    "        0.0643, 0.0469, 0.0318, 0.0343, 0.0303, 0.0034, 0.0037, 0.0001, 0.0095])\n",
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
